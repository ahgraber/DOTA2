{"cells":[{"cell_type":"markdown","source":["## (failed) cross-validation model"],"metadata":{}},{"cell_type":"markdown","source":["Read in data"],"metadata":{}},{"cell_type":"code","source":["# Your answer goes in here\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# main data\n# matchData = sqlContext.sql(\"Select * from matchdata3_csv\") \n\n# lookups \nhero_names = sqlContext.sql(\"Select * from dota_hero_names_csv\")\nitem_ids = sqlContext.sql(\"Select * from dota_item_ids_csv\")\npatch_dates = sqlContext.sql(\"Select * from dota_patch_dates_csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# recreate matchData from scratch\nmatches = sqlContext.sql(\"Select * from match_csv\")\nplayers = sqlContext.sql(\"Select * from players_csv\")\ntime = sqlContext.sql(\"Select * from player_time_csv\")\n\nfrom pyspark.sql.functions import *\n\n# join players with match\njoined = matches.join(players, ['match_id'])\njoined = joined.select(joined.columns[0:51])\n#joined = joined.where('account_id > 0')\n#joined = joined.withColumn('side', when(joined['player_slot'] < 100, \"Radiant\").otherwise(\"Dire\"))\n\n#joined.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# spread by player_slot & hero_id\n\ntemp = joined.select('match_id', 'duration', 'first_blood_time', 'hero_id', 'player_slot', 'radiant_win')\npivot = (temp\n         .select('match_id', 'duration', 'first_blood_time', 'hero_id', 'player_slot', 'radiant_win')\n         .groupBy('match_id')\n         .pivot('player_slot', ['0','1','2','3','4','128','129','130','131','132'])\n         .agg(first('hero_id')))\n\npivot = (pivot\n         .withColumnRenamed('0','t0')\n         .withColumnRenamed('1','t1')\n         .withColumnRenamed('2','t2')\n         .withColumnRenamed('3','t3')\n         .withColumnRenamed('4','t4')\n         .withColumnRenamed('128','t128')\n         .withColumnRenamed('129','t129')\n         .withColumnRenamed('130','t130')\n         .withColumnRenamed('131','t131')\n         .withColumnRenamed('132','t132'))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# build dummy columns per side for each hero_id\n\nstring = \"radiant\"\nfor i in range(114): # 113 heroes\n  pivot=pivot.withColumn(string+str(i), when(((col('t0') == i) | (col('t1') == i) | (col('t2') == i) | (col('t3') == i) | (col('t4') == i)), 1)\n                         .otherwise(0))\n\nstring = \"dire\"\nfor i in range(114): # 113 heroes\n  pivot=pivot.withColumn(string+str(i), when(((col('t128') == i) | (col('t129') == i) | (col('t130') == i) | (col('t131') == i) | (col('t132') == i)), 1)\n                         .otherwise(0))\n  \n#pivot.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# subset important cols\ndata = matches.select('match_id', 'duration', 'first_blood_time', 'radiant_win')\npivot = pivot.drop('t0','t1','t2','t3','t4','t128','t129','t130','t131','t132')\n\n# join important cols to pivoted/spread data\nmatchData = data.join(pivot, ['match_id'])\n\n#matchData.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# add in data from player_time\nmatchData = matchData.join(time, ['match_id'])\n\n#matchData.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#matchData.groupBy('match_id').count().show()\n# since we get data every 60 seconds, the count here represents roughly how long each match lasted"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# create sum columns for team gold, xp, lh\nmodelData = matchData.withColumn(\"radiant_gold\", \n                                 matchData[\"gold_t_0\"] + matchData[\"gold_t_1\"] + matchData[\"gold_t_2\"] + matchData[\"gold_t_3\"] + matchData[\"gold_t_4\"])\nmodelData = modelData.withColumn(\"radiant_xp\", \n                                 matchData[\"xp_t_0\"] + matchData[\"xp_t_1\"] + matchData[\"xp_t_2\"] + matchData[\"xp_t_3\"] + matchData[\"xp_t_4\"])\nmodelData = modelData.withColumn(\"radiant_lh\", \n                                 matchData[\"lh_t_0\"] + matchData[\"lh_t_1\"] + matchData[\"lh_t_2\"] + matchData[\"lh_t_3\"] + matchData[\"lh_t_4\"])\n\nmodelData = modelData.withColumn(\"dire_gold\", \n                                 matchData[\"gold_t_128\"] + matchData[\"gold_t_129\"] + matchData[\"gold_t_130\"] + matchData[\"gold_t_131\"] + matchData[\"gold_t_132\"])\nmodelData = modelData.withColumn(\"dire_xp\", \n                                 matchData[\"xp_t_128\"] + matchData[\"xp_t_129\"] + matchData[\"xp_t_130\"] + matchData[\"xp_t_131\"] + matchData[\"xp_t_132\"])\nmodelData = modelData.withColumn(\"dire_lh\", \n                                 matchData[\"lh_t_128\"] + matchData[\"lh_t_129\"] + matchData[\"lh_t_130\"] + matchData[\"lh_t_131\"] + matchData[\"lh_t_132\"])"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# create differential columns for gold, xp, lh\nmodelData = modelData.withColumn(\"golddiff\", modelData[\"radiant_gold\"] - modelData[\"dire_gold\"])\nmodelData = modelData.withColumn(\"xpdiff\", modelData[\"radiant_xp\"] - modelData[\"dire_xp\"])\nmodelData = modelData.withColumn(\"lhdiff\", modelData[\"radiant_lh\"] - modelData[\"dire_lh\"])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# recode 'times' to float\nmodelData = modelData.withColumn('times', modelData.times.cast('float'))\n#modelData.dtypes"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# drop unnecessary / old columns\nmodelData = modelData.drop(\"match_id\",\"gold_t_0\", \"gold_t_1\", \"gold_t_2\", \"gold_t_3\", \"gold_t_4\", \n                           \"xp_t_0\", \"xp_t_1\", \"xp_t_2\", \"xp_t_3\", \"xp_t_4\",\n                           \"lh_t_0\", \"lh_t_1\", \"lh_t_2\", \"lh_t_3\", \"lh_t_4\",\n                           \"gold_t_128\", \"gold_t_129\", \"gold_t_130\", \"gold_t_131\", \"gold_t_132\",\n                           \"xp_t_128\", \"xp_t_129\", \"xp_t_130\", \"xp_t_131\", \"xp_t_132\",\n                           \"lh_t_128\", \"lh_t_129\", \"lh_t_130\", \"lh_t_131\", \"lh_t_132\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#print(modelData.columns)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Inputs\n* Heroes (dummy variables)\n* per-minute gold (team sum)\n* per-minute xp (team sum)\n* per-minute last-hits (team sum)\n* final result (coded as 'radiant win')\n\nLogistic regression predicting (radiant-side) win probability given a timestamp and gold, xp, lh\n* this means we may have to subset by timestamp(?)"],"metadata":{}},{"cell_type":"code","source":["# define dependent variable\ntarget = 'radiant_win'"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# recode 'radiant_win' into 0/1\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf # UDF - user defined function\nfrom pyspark.sql.functions import *\n\n# create user defined function that codes as 1 if \"CDK\" and 0 otherwise; cast as type integer\nget_01_label = udf(lambda x: 1 if x == 'True' else 0, IntegerType())\n\n# withColumn(\"new_name\", function(old_df[\"old_name\"]))\nmodelData = modelData.withColumn(\"radiant_win\", get_01_label(modelData[\"radiant_win\"]))\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# step 1 to identify premade onehot encoded heroes (they start with 'radiant' or 'dire')\nrhero_input = [col for col in modelData.columns if ('radiant') in col]\ndhero_input = [col for col in modelData.columns if ('dire') in col]\n\nhero_input = rhero_input + dhero_input\n#hero_input"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# identify the non-hero columns (they start with 'radiant_' or 'dire_')\nrnonhero_input = [col for col in modelData.columns if ('radiant_') in col]\ndnonhero_input = [col for col in modelData.columns if ('dire_') in col]\n\nnonhero_input = rnonhero_input + dnonhero_input\n#nonhero_input"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# select only premade onehot encoded heroes\nhero_input = list(set(hero_input)-set(nonhero_input))\n#hero_input"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["### no categorical variables because we've already one-hot encoded heroes\n#dtypes = modelData.dtypes\n#cat_input = []\n#for i in range(0, len(modelData.columns)):\n#   if dtypes[i][1] == 'string':      # if data type is string\n#      cat_input.append(dtypes[i][0])  # append to list of categorical variables\n#cat_input = list(set(cat_input)-set(target))   # returns elements that exist in list A that are NOT in list B (remove dependent variable from list)\n   \n#cat_input"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# numerical variables\nnum_input = list(set(modelData.columns) - set([target]) - set(hero_input))\n#num_input"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["####Begin building pipeline\nNumeric/Continuous variables"],"metadata":{}},{"cell_type":"code","source":["# impute missing values\nfrom pyspark.ml.feature import Imputer\n\nnumImputer = Imputer(strategy='median', inputCols=num_input, outputCols=num_input)  # default is mean imputation\n#test = numImputer.fit(modelData).transform(modelData)\n#test.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nnumAssembler = VectorAssembler(inputCols= num_input, outputCol=\"features\")\n#test = numAssembler.transform(modelData)\n#test.select(\"features\").show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["### run scaler to transform all columns in num_input\n#minmax = [MinMaxStandardizer( inputCol = column, outputCol = column+\"_standardized\") for column in num_input]\n\nfrom pyspark.ml.feature import MinMaxScaler\n\nminmax = MinMaxScaler(inputCol = \"features\", outputCol = \"scaled\") \n#test = minmax.fit(test).transform(test)\n#test.select(\"features\",\"scaled\").show()\n#test.select(\"features\",\"scaled\").head()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Add pre-coded categorical variables"],"metadata":{}},{"cell_type":"code","source":["input_cols = [\"scaled\"]\nfor i in hero_input:\n  input_cols.append(i)\n#input_cols"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nfinalAssembler = VectorAssembler(inputCols= input_cols, outputCol=\"finalFeatures\")\n#test=finalAssembler.transform(test)\n#test.select(\"features\",\"finalFeatures\").head()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol = \"finalFeatures\", \n                                    labelCol = target , \n                                    maxIter=10)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["### staging area\n\nstages = []\nstages.append(numImputer)\nstages.append(numAssembler)\nstages.append(minmax)\nstages.append(finalAssembler)\nstages.append(lr) # could run this post-regression\n\nstages"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["### Set up the ML Pipeline\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=stages)  # with logit\npipeline2 = Pipeline(stages=[numImputer, numAssembler, minmax, finalAssembler]) # separate logit from data transforms"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["### break the pipeline between data setup and regression - data setup\n\npipedData = pipeline2.fit(modelData).transform(modelData)\n#pipedData.show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["### convert to RDD to train/test split\nrdd = pipedData.rdd\n\n# Split data into training (10%) and test (90%)\ntrain, test = rdd.randomSplit([0.1, 0.9])"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["trainDF = train.toDF()\ntestDF = test.toDF()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["### break the pipeline between data setup and regression - regression part\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n    .build()\n    \n    \ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(labelCol=target), \n                          numFolds=2)  # 2-fold cross-validation\n\n# run the crossvalidation\ncvModel = crossval.fit(trainDF)\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["bestModel = cvModel.bestModel\nbestParams = bestModel.extractParamMap()\nbestParams\n# regParam = .75\n# elasticNetParam = .25"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nbestModel = LogisticRegression(featuresCol = \"finalFeatures\", \n                                    labelCol = target , \n                                    maxIter=10,\n                                    regParam = 0.25,\n                                    elasticNetParam = 0.75)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#val lrmodel = bestModel.asInstanceOf[LogisticRegressionModel]\nbestModel.fit(trainDF).coefficients #or weights?\nbestModel.fit(trainDF).intercept"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["trainResults = bestModel.fit(trainDF).transform(trainDF)\ntestResults = bestModel.fit(testDF).transform(testDF)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["print(trainResults.columns)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# classification evaluation metrics - training set\neval = trainResults.select(target,'prediction')\n\nTP = eval[(eval[target] == 1) & (eval['prediction'] == 1)].count()\nFP = eval[(eval[target] == 0) & (eval['prediction'] == 1)].count()\nTN = eval[(eval[target] == 0) & (eval['prediction'] == 0)].count()\nFN = eval[(eval[target] == 1) & (eval['prediction'] == 0)].count()\n\nprint(\"True Positives:\", TP)\nprint(\"False Positives:\", FP)\nprint(\"True Negatives:\", TN)\nprint(\"False Negatives:\", FN)\nprint(\"Total:\", eval.count())\n\n# accuracy rate, false positive rate, false negative rate\nA = float(TP + TN) / (TP + TN + FP + FN)\nprint(\"accuracy:\", A)\n\nP = float(TP) / (TP + FP)\nprint(\"precision:\", P)\n\nR = float(TP) / (TP + FN)\nprint(\"recall(TPR):\", R)\n\nS = float(TN) / (TN + FP)\nprint(\"specificity(TNR):\", S)\n\nN = float(FN) / (TP + FN)\nprint(\"FNR:\", N)\n\nF = float(FP) / (FP + TN)\nprint(\"FPR:\", F)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# classification evaluation metrics - testing set\neval = testResults.select(target,'prediction')\n\nTP = eval[(eval[target] == 1) & (eval['prediction'] == 1)].count()\nFP = eval[(eval[target] == 0) & (eval['prediction'] == 1)].count()\nTN = eval[(eval[target] == 0) & (eval['prediction'] == 0)].count()\nFN = eval[(eval[target] == 1) & (eval['prediction'] == 0)].count()\n\nprint(\"True Positives:\", TP)\nprint(\"False Positives:\", FP)\nprint(\"True Negatives:\", TN)\nprint(\"False Negatives:\", FN)\nprint(\"Total:\", eval.count())\n\n# accuracy rate, false positive rate, false negative rate\nA = float(TP + TN) / (TP + TN + FP + FN)\nprint(\"accuracy:\", A)\n\nP = float(TP) / (TP + FP)\nprint(\"precision:\", P)\n\nR = float(TP) / (TP + FN)\nprint(\"recall(TPR):\", R)\n\nS = float(TN) / (TN + FP)\nprint(\"specificity(TNR):\", S)\n\nN = float(FN) / (TP + FN)\nprint(\"FNR:\", N)\n\nF = float(FP) / (FP + TN)\nprint(\"FPR:\", F)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# calculate the fpr and tpr for all thresholds of the classification\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\n\nrocauc = eval.toPandas()\n\nfpr, tpr, thresholds = metrics.roc_curve(rocauc[target], rocauc['prediction'])\nroc_auc = metrics.auc(fpr, tpr)\n\n# ROC\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5, 5))\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n#plt.show()\ndisplay(f)\n"],"metadata":{},"outputs":[],"execution_count":44}],"metadata":{"name":"DOTA2 failed cv model","notebookId":4333913922897619},"nbformat":4,"nbformat_minor":0}
