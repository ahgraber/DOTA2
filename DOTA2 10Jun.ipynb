{"cells":[{"cell_type":"markdown","source":["#DOTA 2 ML in Spark"],"metadata":{}},{"cell_type":"markdown","source":["Read in data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# main data\n# matchData = sqlContext.sql(\"Select * from matchdata3_csv\") \n\n# lookups \n#hero_names = sqlContext.sql(\"Select * from hero_names_csv\")\n#item_ids = sqlContext.sql(\"Select * from item_ids_csv\")\n#patch_dates = sqlContext.sql(\"Select * from dota_patch_dates_csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# recreate matchData from scratch\nmatches = sqlContext.sql(\"Select * from match_csv\")\nplayers = sqlContext.sql(\"Select * from players_csv\")\ntime = sqlContext.sql(\"Select * from player_time_csv\")\n\nfrom pyspark.sql.functions import *\n\n# join players with match\njoined = matches.join(players, ['match_id'])\njoined = joined.select(joined.columns[0:51])\n#joined = joined.where('account_id > 0')\n#joined = joined.withColumn('side', when(joined['player_slot'] < 100, \"Radiant\").otherwise(\"Dire\"))\n\njoined.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# spread by player_slot & hero_id\n\ntemp = joined.select('match_id', 'duration', 'first_blood_time', 'hero_id', 'player_slot', 'radiant_win')\npivot = (temp\n         .select('match_id', 'duration', 'first_blood_time', 'hero_id', 'player_slot', 'radiant_win')\n         .groupBy('match_id')\n         .pivot('player_slot', ['0','1','2','3','4','128','129','130','131','132'])\n         .agg(first('hero_id')))\n\npivot = (pivot\n         .withColumnRenamed('0','t0')\n         .withColumnRenamed('1','t1')\n         .withColumnRenamed('2','t2')\n         .withColumnRenamed('3','t3')\n         .withColumnRenamed('4','t4')\n         .withColumnRenamed('128','t128')\n         .withColumnRenamed('129','t129')\n         .withColumnRenamed('130','t130')\n         .withColumnRenamed('131','t131')\n         .withColumnRenamed('132','t132'))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# build dummy columns per side for each hero_id\n\nstring = \"radiant\"\nfor i in range(114): # 113 heroes\n  pivot=pivot.withColumn(string+str(i), when(((col('t0') == i) | (col('t1') == i) | (col('t2') == i) | (col('t3') == i) | (col('t4') == i)), 1)\n                         .otherwise(0))\n\nstring = \"dire\"\nfor i in range(114): # 113 heroes\n  pivot=pivot.withColumn(string+str(i), when(((col('t128') == i) | (col('t129') == i) | (col('t130') == i) | (col('t131') == i) | (col('t132') == i)), 1)\n                         .otherwise(0))\n  \npivot.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# subset important cols\ndata = matches.select('match_id', 'duration', 'first_blood_time', 'radiant_win')\npivot = pivot.drop('t0','t1','t2','t3','t4','t128','t129','t130','t131','t132')\n\n# join important cols to pivoted/spread data\nmatchData = data.join(pivot, ['match_id'])\n\nmatchData.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# add in data from player_time\nmatchData = matchData.join(time, ['match_id'])\n\nmatchData.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["matchData.groupBy('match_id').count().show()\n# since we get data every 60 seconds, the count here represents roughly how long each match lasted"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# create sum columns for team gold, xp, lh\nmodelData = matchData.withColumn(\"radiant_gold\", \n                                 matchData[\"gold_t_0\"] + matchData[\"gold_t_1\"] + matchData[\"gold_t_2\"] + matchData[\"gold_t_3\"] + matchData[\"gold_t_4\"])\nmodelData = modelData.withColumn(\"radiant_xp\", \n                                 matchData[\"xp_t_0\"] + matchData[\"xp_t_1\"] + matchData[\"xp_t_2\"] + matchData[\"xp_t_3\"] + matchData[\"xp_t_4\"])\nmodelData = modelData.withColumn(\"radiant_lh\", \n                                 matchData[\"lh_t_0\"] + matchData[\"lh_t_1\"] + matchData[\"lh_t_2\"] + matchData[\"lh_t_3\"] + matchData[\"lh_t_4\"])\n\nmodelData = modelData.withColumn(\"dire_gold\", \n                                 matchData[\"gold_t_128\"] + matchData[\"gold_t_129\"] + matchData[\"gold_t_130\"] + matchData[\"gold_t_131\"] + matchData[\"gold_t_132\"])\nmodelData = modelData.withColumn(\"dire_xp\", \n                                 matchData[\"xp_t_128\"] + matchData[\"xp_t_129\"] + matchData[\"xp_t_130\"] + matchData[\"xp_t_131\"] + matchData[\"xp_t_132\"])\nmodelData = modelData.withColumn(\"dire_lh\", \n                                 matchData[\"lh_t_128\"] + matchData[\"lh_t_129\"] + matchData[\"lh_t_130\"] + matchData[\"lh_t_131\"] + matchData[\"lh_t_132\"])"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# create differential columns for gold, xp, lh\nmodelData = modelData.withColumn(\"golddiff\", modelData[\"radiant_gold\"] - modelData[\"dire_gold\"])\nmodelData = modelData.withColumn(\"xpdiff\", modelData[\"radiant_xp\"] - modelData[\"dire_xp\"])\nmodelData = modelData.withColumn(\"lhdiff\", modelData[\"radiant_lh\"] - modelData[\"dire_lh\"])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# recode 'times' to float\nmodelData = modelData.withColumn('times', modelData.times.cast('float'))\n#modelData.dtypes"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# drop unnecessary / old columns\nmodelData = modelData.drop(\"match_id\",\"gold_t_0\", \"gold_t_1\", \"gold_t_2\", \"gold_t_3\", \"gold_t_4\", \n                           \"xp_t_0\", \"xp_t_1\", \"xp_t_2\", \"xp_t_3\", \"xp_t_4\",\n                           \"lh_t_0\", \"lh_t_1\", \"lh_t_2\", \"lh_t_3\", \"lh_t_4\",\n                           \"gold_t_128\", \"gold_t_129\", \"gold_t_130\", \"gold_t_131\", \"gold_t_132\",\n                           \"xp_t_128\", \"xp_t_129\", \"xp_t_130\", \"xp_t_131\", \"xp_t_132\",\n                           \"lh_t_128\", \"lh_t_129\", \"lh_t_130\", \"lh_t_131\", \"lh_t_132\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["print(modelData.columns)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Research Question: What wins?  \n* Do heroes matter?  \n* Do items matter?  \n* Does KDA matter?  \n* Pattern Frequency - draft predictions --> https://spark.apache.org/docs/2.2.0/ml-frequent-pattern-mining.html"],"metadata":{}},{"cell_type":"markdown","source":["## Inputs\n* Heroes (dummy variables)\n* per-minute gold (team sum)\n* per-minute xp (team sum)\n* per-minute last-hits (team sum)\n* final result (coded as 'radiant win')\n\nLogistic regression predicting (radiant-side) win probability given a timestamp and gold, xp, lh\n* this means we may have to subset by timestamp(?)"],"metadata":{}},{"cell_type":"markdown","source":["__TASK 1:__ Read the dataset and Encode the class column to 0 and 1"],"metadata":{}},{"cell_type":"code","source":["# define dependent variable\ntarget = 'radiant_win'"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# recode 'radiant_win' into 0/1\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf # UDF - user defined function\nfrom pyspark.sql.functions import *\n\n# create user defined function that codes as 1 if \"CDK\" and 0 otherwise; cast as type integer\nget_01_label = udf(lambda x: 1 if x == 'True' else 0, IntegerType())\n\n# withColumn(\"new_name\", function(old_df[\"old_name\"]))\nmodelData = modelData.withColumn(\"radiant_win\", get_01_label(modelData[\"radiant_win\"]))\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print(modelData.columns)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# premade onehot encoded heroes\nrhero_input = [col for col in modelData.columns if ('radiant') in col]\ndhero_input = [col for col in modelData.columns if ('dire') in col]\n\nhero_input = rhero_input + dhero_input\nhero_input"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["rnonhero_input = [col for col in modelData.columns if ('radiant_') in col]\ndnonhero_input = [col for col in modelData.columns if ('dire_') in col]\n\nnonhero_input = rnonhero_input + dnonhero_input\nnonhero_input"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# premade onehot encoded heroes\nhero_input = list(set(hero_input)-set(nonhero_input))\nhero_input"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# categorical variables\ndtypes = modelData.dtypes\ncat_input = []\nfor i in range(0, len(modelData.columns)):\n   if dtypes[i][1] == 'string':      # if data type is string\n      cat_input.append(dtypes[i][0])  # append to list of categorical variables\ncat_input = list(set(cat_input)-set(target))   # returns elements that exist in list A that are NOT in list B (remove dependent variable from list)\n   #\ncat_input"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# numerical variables\nnum_input = list(set(modelData.columns) - set([target]) - set(cat_input) - set(hero_input))\nnum_input"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["__TASK 2:__ Do some visualizations to summarize your dataset"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  #ImportError: cannot import name '_to_unmasked_float_array'\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# take a sample of the total data, convert to pandas df for plotting\nmodelData_pd = modelData.sample(False, .25, 12345).toPandas()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# calculate the correlation matrix\ncorr = modelData_pd.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 20))\n\n# plot the heatmap\nrc={'font.size': 18, 'axes.labelsize': 18, 'legend.fontsize': 18.0, \n    'axes.titlesize': 18, 'xtick.labelsize': 12, 'ytick.labelsize': 12}\n\nsns.set(rc=rc)\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\n\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# subset data\nwins = modelData_pd.loc[modelData_pd['radiant_win'] == 1]\nloses = modelData_pd.loc[modelData_pd['radiant_win'] == 0]"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(5, 4))\n\nrc = {'font.size': 18}\nsns.set(font_scale=1, rc=rc)\n\n# density plots\nsns.kdeplot(wins['golddiff'], ax=ax)\nsns.kdeplot(loses['golddiff'], ax=ax)\n\n# replace labels\n#new_labels = ['radiant_win', 'radiant_lose']\n#for t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(6, 3))\n\nrc = {'font.size': 18}\nsns.set(font_scale=1, rc=rc)\n\n# density plots\nsns.kdeplot(wins['xpdiff'], ax=ax)\nsns.kdeplot(loses['golddiff'], ax=ax)\n\n# replace labels\n#new_labels = ['radiant_win', 'radiant_lose']\n#for t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n  \ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(5, 4))\n\nrc = {'font.size': 18}\nsns.set(font_scale=1, rc=rc)\n\n# density plots\nsns.kdeplot(wins['lhdiff'], ax=ax)\nsns.kdeplot(loses['lhdiff'], ax=ax)\n\n# replace labels\n#new_labels = ['radiant_win', 'radiant_lose']\n#for t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n  \ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["__TASK 3:__ Write a transfomation class and its functions for replacing numerical missing values with the median. (You will write a trasformer class). Create the transformers for numerical variables using the transformation class you have just written"],"metadata":{}},{"cell_type":"code","source":["# impute missing values\nfrom pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import col\n\n# class ClassName (SuperClass, SuperClass, ...)\n# inherits functions from SuperClasses\nclass NumericImputer(Transformer, HasInputCol, HasOutputCol):\n\n      # 'self' refers to future objects created using this class\n      def __init__(self, inputCol=None, outputCol=None):\n            super(NumericImputer, self).__init__() # creates the NumericImputer object\n            self.setParams(inputCol = inputCol , outputCol = outputCol) #from Transformer superclass\n            # .setParams(Transformer.inputCol = NumericImputer.inputCol, ...)\n\n      # typically have 'setters' and 'getters\n      def setParams(self, inputCol=None, outputCol=None):\n         return self._set(inputCol = inputCol, outputCol = outputCol)\n            \n\n      # update _transform any time we need a custom transformer!\n      def _transform(self, dataset):\n         from pyspark.sql.functions import when   \n         in_col = self.getInputCol()\n         out_col = self.getOutputCol()\n         median_v = dataset.approxQuantile(in_col, [0.5], 0)[0] # get the median of the input column\n         #dataset = dataset.fillna(median_v, subset=in_col)\n         return dataset.withColumn(out_col, when(col(in_col).isNull(), median_v).otherwise(col(in_col))) # replace null with median, otherwise bring through input"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["numericimputers = [NumericImputer( inputCol = column, outputCol = column) for column in num_input]\n# run imputer transform for all variables in our list of numerical input variables 'num_input'"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["__TASK 4:__ Write a transfomation class and its functions for replacing categorical missing values with the mode. (You will write a trasformer class). Create the transformers for categorical variables using the transformation class you have just written"],"metadata":{}},{"cell_type":"code","source":["# impute missing values\nfrom pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import col\n\n# class ClassName (SuperClass, SuperClass, ...)\n# inherits functions from SuperClasses\nclass CategoricalImputer(Transformer, HasInputCol, HasOutputCol):\n\n      # 'self' refers to future objects created using this class\n      def __init__(self, inputCol=None, outputCol=None):\n            super(CategoricalImputer, self).__init__() # creates the NumericImputer object\n            self.setParams(inputCol = inputCol , outputCol = outputCol) #from Transformer superclass\n            # .setParams(Transformer.inputCol = NumericImputer.inputCol, ...)\n\n      # typically have 'setters' and 'getters\n      def setParams(self, inputCol=None, outputCol=None):\n         return self._set(inputCol = inputCol, outputCol = outputCol)\n            \n\n      # update _transform any time we need a custom transformer!\n      def _transform(self, dataset):\n         from pyspark.sql.functions import when   \n         in_col = self.getInputCol()\n         out_col = self.getOutputCol()\n         \n         # group by the unique values in in_col; count; return most frequent\n         mode_v = dataset.select(in_col).groupBy(col(in_col)).count().sort(col('count').asc()).collect().pop()[0]\n         return dataset.withColumn(out_col, when(col(in_col).isNull(), mode_v).otherwise(col(in_col))) \n         # replace null with mode, otherwise bring through input"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["categoricalimputers = [CategoricalImputer( inputCol = column, outputCol = column) for column in cat_input]\n# run imputer transform for all variables in our list of categorical input variables 'cat_input'"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["__TASK 5:__ Write a transfomation class and its functions for normalizing numerical data. The _transform function should normalize your column using the feature scaling method: (x-xmin)/(xmax-xmin)\n\nCreate the transformers for numerical variables using the transformation class you have just written"],"metadata":{}},{"cell_type":"code","source":["# min-max scale a dataset\n\nfrom pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\n\n\nclass MinMaxStandardizer(Transformer, HasInputCol, HasOutputCol):\n\n      def __init__(self, inputCol=None, outputCol=None):\n            super(MinMaxStandardizer, self).__init__()\n            self.setParams(inputCol = inputCol , outputCol = outputCol)\n\n            \n            \n      def setParams(self, inputCol=None, outputCol=None):\n         return self._set(inputCol = inputCol, outputCol = outputCol)\n            \n\n      def _transform(self, dataset):\n         from pyspark.sql.functions import stddev, mean, col\n         in_col = dataset[self.getInputCol()]\n         out_col = self.getOutputCol()\n         # update below to tweak\n         minimum, maximum = dataset.select(F.min(in_col),  F.max(in_col)).first()\n         return dataset.withColumn(out_col, (in_col - minimum)/(maximum-minimum))    \n      \n      \n# replicate transformation class; tweak to use (x-min)/range (or use minmax scaler)\n# create transformer for each numerical varialbe"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# run scaler to transform all columns in num_input\nminmax = [MinMaxStandardizer( inputCol = column, outputCol = column+\"_standardized\") for column in num_input]\n\n#from pyspark.ml.feature import MinMaxScaler\n#minmax = [MinMaxScaler( inputCol = column, outputCol = column+\"_standardized\") for column in num_input]\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["__TASK 6:__ Create the transformers for encoding dummy variables"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\n\n### for categorical\nindexers = [StringIndexer(inputCol = column, outputCol = column+\"_index\") for column in cat_input] \n   # factorize: replace levels of a categorical variable w/ a number\n   # most frequent level is 0, 2nd most frequent is 1, etc...\n\nencoders = [OneHotEncoder(inputCol = column+\"_index\", outputCol = column+\"_dummy\") for column in cat_input] \n   # create binary vector out of category codes (if not ordinal)\n"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["input_cols = []\nfor i in cat_input:\n   input_cols.append(i+\"_dummy\")  # rename in input_cols because we did index/OneHotEncoder and renamed the columns, otherwise it'll use the original categoricals\nfor i in num_input:\n   input_cols.append(i+\"_standardized\") # rename in input_cols because we imputed and standardized and renamed the columns, otherwise it'll use original numerics\ninput_cols"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(inputCols= input_cols, outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["__TASK 7:__ Create the transformer for logistic regression (or decision tree or random forest)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol = 'features', \n                                    labelCol = target , \n                                    maxIter=10) # use grid search to optimize params\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["__Task 8:__ Combine the transfomers and create the pipeline and fit the pipeline."],"metadata":{}},{"cell_type":"code","source":["### staging area\n# this is all python, not pyspark or spark\nimport functools \nimport operator\n\nstages = []\nstages = functools.reduce(operator.concat, [numericimputers, \n                                            minmax,\n                                            categoricalimputers,\n                                            indexers, \n                                            encoders]) # could run this above\nstages.append(assembler) # could run this post assembler\nstages.append(lr) # could run this post-regression\nstages"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["### staging area\n# this is all python, not pyspark or spark\nimport functools \nimport operator\n\n# stages without imputers (because we don't need imputers for our data)\nstages = []\nstages = functools.reduce(operator.concat, [minmax,\n                                            indexers, \n                                            encoders]) # could run this above\nstages.append(assembler) # could run this post assembler\nstages.append(lr) # could run this post-regression\nstages\n# if you look, we have a ton of MinMaxStandardizers - I think this is because we're standardizing the dummy variables we created for the heroes - \n# shouldn't change the data, but might add to the runtime"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["### simplified data\nrdd = modelData.rdd\n\ntrain, test = rdd.randomSplit([.25,.75])\n\ntraindf = train.toDF()\ntestdf = test.toDF()\n\nfrom pyspark.ml import Pipeline\n\n# run the pipeline and fit the model on training data\npipeline = Pipeline(stages=stages)  # create the pipeline\nmodel = pipeline.fit(traindf)# fit the data . transform the data"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# use our model object to predict win probabilities for training and testing sets\ntrainOut = model.transform(traindf)\ntestOut = model.transform(testdf)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["__Task 9:__ Performance checks\n* Write code to calculate accuracy rate, false positive rate, false negative rate\n* Write code to visualize and display ROC (this is called ROC) (can use mathplotlib or seaborne in python to viz)"],"metadata":{}},{"cell_type":"code","source":["# classification evaluation metrics\neval = trainOut.select('radiant_win','prediction')\n\nTP = eval[(eval['radiant_win'] == 1) & (eval['prediction'] == 1)].count()\nFP = eval[(eval['radiant_win'] == 0) & (eval['prediction'] == 1)].count()\nTN = eval[(eval['radiant_win'] == 0) & (eval['prediction'] == 0)].count()\nFN = eval[(eval['radiant_win'] == 1) & (eval['prediction'] == 0)].count()\n\nprint(\"True Positives:\", TP)\nprint(\"False Positives:\", FP)\nprint(\"True Negatives:\", TN)\nprint(\"False Negatives:\", FN)\nprint(\"Total:\", eval.count())\n\n# accuracy rate, false positive rate, false negative rate\nA = float(TP + TN) / (TP + TN + FP + FN)\nprint(\"accuracy:\", A)\n\nP = float(TP) / (TP + FP)\nprint(\"precision:\", P)\n\nR = float(TP) / (TP + FN)\nprint(\"recall(TPR):\", R)\n\nS = float(TN) / (TN + FP)\nprint(\"specificity(TNR):\", S)\n\nN = float(FN) / (TP + FN)\nprint(\"FNR:\", N)\n\nF = float(FP) / (FP + TN)\nprint(\"FPR:\", F)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# classification evaluation metrics\neval = testOut.select('radiant_win','prediction')\n\nTP = eval[(eval['radiant_win'] == 1) & (eval['prediction'] == 1)].count()\nFP = eval[(eval['radiant_win'] == 0) & (eval['prediction'] == 1)].count()\nTN = eval[(eval['radiant_win'] == 0) & (eval['prediction'] == 0)].count()\nFN = eval[(eval['radiant_win'] == 1) & (eval['prediction'] == 0)].count()\n\nprint(\"True Positives:\", TP)\nprint(\"False Positives:\", FP)\nprint(\"True Negatives:\", TN)\nprint(\"False Negatives:\", FN)\nprint(\"Total:\", eval.count())\n\n# accuracy rate, false positive rate, false negative rate\nA = float(TP + TN) / (TP + TN + FP + FN)\nprint(\"accuracy:\", A)\n\nP = float(TP) / (TP + FP)\nprint(\"precision:\", P)\n\nR = float(TP) / (TP + FN)\nprint(\"recall(TPR):\", R)\n\nS = float(TN) / (TN + FP)\nprint(\"specificity(TNR):\", S)\n\nN = float(FN) / (TP + FN)\nprint(\"FNR:\", N)\n\nF = float(FP) / (FP + TN)\nprint(\"FPR:\", F)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# calculate the fpr and tpr for all thresholds of the classification\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\n\nrocauc = eval.toPandas()\n\nfpr, tpr, thresholds = metrics.roc_curve(rocauc['radiant_win'], rocauc['prediction'])\nroc_auc = metrics.auc(fpr, tpr)\n\n# ROC\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5, 5))\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n#plt.show()\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["trainOut.select('prediction').show()"],"metadata":{},"outputs":[],"execution_count":58}],"metadata":{"name":"DOTA2","notebookId":3307212054624866},"nbformat":4,"nbformat_minor":0}
